"""
Chat API routes with Temporal streaming.

Uses Query polling pattern to stream tokens from workflow state.
"""

import asyncio
import logging
import uuid
from collections.abc import AsyncGenerator
from datetime import timedelta

from fastapi import APIRouter, HTTPException, Query
from sse_starlette.sse import EventSourceResponse
from temporalio.client import WorkflowExecutionStatus, WorkflowHandle
from temporalio.exceptions import WorkflowAlreadyStartedError

from backend.temporal.workflows import (
    ChatRequest,
    LLMChatWorkflow,
    StreamState,
)


logger = logging.getLogger(__name__)

router = APIRouter()


async def poll_workflow_stream(
    handle: WorkflowHandle,
    poll_interval: float = 0.15,
    timeout: float = 60.0,
) -> AsyncGenerator[str, None]:
    """
    Poll workflow state and stream tokens.

    Args:
        handle: Workflow handle
        poll_interval: Seconds between polls (default: 150ms)
        timeout: Total timeout in seconds (default: 60s)

    Yields:
        Tokens and status updates from workflow

    Raises:
        TimeoutError: If workflow doesn't complete within timeout
        RuntimeError: If workflow fails
    """
    seen_tokens = 0
    last_status = ""
    start_time = asyncio.get_event_loop().time()

    try:
        while True:
            elapsed = asyncio.get_event_loop().time() - start_time
            if elapsed > timeout:
                try:
                    await handle.cancel()
                    logger.warning(f"Workflow cancelled due to timeout: {handle.id}")
                except Exception as e:
                    logger.error(f"Failed to cancel workflow: {e}")

                raise TimeoutError(
                    f"Workflow timed out after {timeout} seconds. "
                    "The workflow has been cancelled."
                )

            try:
                state_dict = await handle.query("get_stream_state")
                if isinstance(state_dict, dict):
                    state = StreamState(**state_dict)
                else:
                    state = state_dict
            except Exception as e:
                logger.error(f"Query failed: {e}")
                raise RuntimeError(f"Failed to query workflow state: {e}")

            new_tokens = state.tokens[seen_tokens:]
            for token in new_tokens:
                yield token

            seen_tokens += len(new_tokens)

            if state.status != last_status:
                yield f"__STATUS__: {state.status}"
                last_status = state.status

            if state.completed:
                if state.error:
                    yield f"__ERROR__: {state.error}"
                    raise RuntimeError(f"Workflow failed: {state.error}")
                else:
                    yield "__DONE__"
                    logger.info(f"Stream completed: {seen_tokens} tokens")
                    break

            await asyncio.sleep(poll_interval)

    except asyncio.CancelledError:
        logger.info(f"Client disconnected, cancelling workflow: {handle.id}")
        try:
            await handle.cancel()
        except Exception as e:
            logger.warning(f"Failed to cancel workflow: {e}")
        raise


@router.get("/chat")
async def chat_stream(
    query: str = Query(..., min_length=1, description="User's question about documents"),
    doc_path: str = Query(
        "/", description="Path relative to Desktop (use '/' for Desktop root)"
    ),
    max_tokens: int = Query(4096, description="Maximum tokens to generate"),
    temperature: float = Query(
        0.7, ge=0.0, le=1.0, description="LLM temperature"
    ),
    timeout: int = Query(
        60, ge=10, le=300, description="Timeout in seconds (10-300)"
    ),
) -> EventSourceResponse:
    """
    Stream chat response via Server-Sent Events.

    Tokens are streamed in real-time as they're generated by the LLM.
    Uses Temporal Query polling to fetch tokens from workflow state.

    Args:
        query: User's question or query
        doc_path: Path relative to Desktop
        max_tokens: Maximum tokens for LLM to generate
        temperature: Sampling temperature (0.0 = deterministic, 1.0 = creative)
        timeout: Workflow timeout in seconds

    Returns:
        Server-Sent Events stream with LLM tokens and status updates

    Special messages in stream:
        - `__STATUS__: <message>` - Progress updates
        - `__ERROR__: <message>` - Error occurred
        - `__DONE__` - Stream completed successfully

    Raises:
        HTTPException: If workflow fails to start
    """
    from backend.api.main import temporal_client

    if temporal_client is None:
        raise HTTPException(
            status_code=503, detail="Service unavailable: Temporal not connected"
        )

    # Normalize common Desktop/Documents path formats to "/"
    normalized_path = doc_path.strip()

    if normalized_path in ["~", "~/", "~/Desktop", "~/Documents"]:
        normalized_path = "/"
    elif normalized_path.startswith("~/Desktop/"):
        normalized_path = normalized_path.replace("~/Desktop/", "")
    elif normalized_path.startswith("~/Documents/"):
        normalized_path = normalized_path.replace("~/Documents/", "")
    elif normalized_path.startswith("~"):
        raise HTTPException(
            status_code=400,
            detail="Invalid path. Use '/' for Desktop root or 'subfolder' for subdirectories.",
        )

    if normalized_path.startswith("/documents"):
        normalized_path = normalized_path.replace("/documents", "", 1)
        if not normalized_path:
            normalized_path = "/"

    workflow_id = f"chat-{uuid.uuid4()}"

    logger.info(
        f"Starting chat: workflow_id={workflow_id}, "
        f"query='{query[:50]}...', path='{normalized_path}', "
        f"timeout={timeout}s"
    )

    try:
        handle = await temporal_client.start_workflow(
            LLMChatWorkflow.run,
            ChatRequest(
                user_query=query,
                doc_path=normalized_path,
                llm_max_tokens=max_tokens,
                llm_temperature=temperature,
            ),
            id=workflow_id,
            task_queue="chat-queue",
            execution_timeout=timedelta(seconds=timeout + 10),
        )

        logger.info(f"Workflow started: {workflow_id}")

        async def event_generator() -> AsyncGenerator[str, None]:
            """Generate SSE events from workflow state."""
            try:
                async for message in poll_workflow_stream(
                    handle, poll_interval=0.15, timeout=timeout
                ):
                    yield message

            except TimeoutError as e:
                logger.error(f"Timeout: {e}")
                yield f"__ERROR__: {e}"

            except RuntimeError as e:
                logger.error(f"Runtime error: {e}")
                yield f"__ERROR__: {e}"

            except Exception as e:
                logger.error(f"Stream error: {e}")
                yield f"__ERROR__: {e}"

        return EventSourceResponse(event_generator())

    except WorkflowAlreadyStartedError:
        raise HTTPException(
            status_code=409, detail=f"Workflow already started: {workflow_id}"
        )

    except Exception as e:
        logger.error(f"Failed to start workflow: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to start chat workflow: {e!s}"
        )


@router.get("/chat/status/{workflow_id}")
async def get_workflow_status(workflow_id: str):
    """
    Get status of a chat workflow.

    Args:
        workflow_id: Workflow ID to check

    Returns:
        Workflow status information and current stream state

    Raises:
        HTTPException: If workflow not found
    """
    from backend.api.main import temporal_client

    if temporal_client is None:
        raise HTTPException(
            status_code=503, detail="Service unavailable: Temporal not connected"
        )

    try:
        handle = temporal_client.get_workflow_handle(workflow_id)
        desc = await handle.describe()

        state = None
        if desc.status == WorkflowExecutionStatus.RUNNING:
            try:
                state_dict = await handle.query("get_stream_state")
                if isinstance(state_dict, dict):
                    state = StreamState(**state_dict)
                else:
                    state = state_dict
            except Exception as e:
                logger.warning(f"Failed to query state: {e}")

        return {
            "workflow_id": workflow_id,
            "status": desc.status.name,
            "start_time": desc.start_time.isoformat() if desc.start_time else None,
            "close_time": desc.close_time.isoformat() if desc.close_time else None,
            "stream_state": {
                "tokens_count": len(state.tokens) if state else 0,
                "status": state.status if state else None,
                "completed": state.completed if state else False,
                "error": state.error if state else None,
            }
            if state
            else None,
        }

    except Exception as e:
        logger.error(f"Failed to get workflow status: {e}")
        raise HTTPException(status_code=404, detail=f"Workflow not found: {workflow_id}")


@router.post("/chat/cancel/{workflow_id}")
async def cancel_workflow(workflow_id: str):
    """
    Cancel a running chat workflow.

    Args:
        workflow_id: Workflow ID to cancel

    Returns:
        Cancellation confirmation

    Raises:
        HTTPException: If workflow not found or cancellation fails
    """
    from backend.api.main import temporal_client

    if temporal_client is None:
        raise HTTPException(
            status_code=503, detail="Service unavailable: Temporal not connected"
        )

    try:
        handle = temporal_client.get_workflow_handle(workflow_id)
        await handle.cancel()

        logger.info(f"Workflow cancelled: {workflow_id}")

        return {
            "workflow_id": workflow_id,
            "status": "cancelled",
            "message": "Workflow cancellation requested",
        }

    except Exception as e:
        logger.error(f"Failed to cancel workflow: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to cancel workflow: {e!s}"
        )
